{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Processing assignment 3: Word embeddings and society\n",
    "\n",
    "In this assignment you will have to load vectorial representations of words and calculate their cosine similarity, a common distance metric to evaluate semantic similarity.\n",
    "\n",
    "On grading: There are six exercises in this assignment. You must have at least three correct exercises (and among the incorrect ones, there should be some proper attempt to solve the missing exercises). What we mean is that if you do three perfect exercises but the remaining two exercises are blank, the assignment will not be considered passed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Exercise 1:\n",
    " \n",
    "In order to play with word embeddings, we need a way of storing them in our program. We need a data structure to represent all the word embeddings.\n",
    "\n",
    "The goal of this exercise is to open the file where the embeddings are saved and to put them in a variable that you can use afterwards.\n",
    "\n",
    "You can represent the data in the way that you think it fits best. The result can go from a really simple approach until a complex but useful class.\n",
    "\n",
    "Given a word, such as `\"house\"`, this data structure should return the embeddings related to that word.\n",
    "\n",
    "In the saved file, we will have a set of words, and each word will be represented as a sequence of floating point numbers, such as:\n",
    "\n",
    "`house --> 0.001 0.002 0.005 0.001 0.0012312 0.004 ...`\n",
    "\n",
    "`cow --> 0.2 0.01 0.00031 0.01 0.9 0.00031 0.0015 0.002 ...`\n",
    "\n",
    "The number of floating point numbers will always be the same.\n",
    "\n",
    "---\n",
    " \n",
    "If we want to play with word embeddings, we have to get them from somewhere. Pick English embeddings from [Absalon](https://absalon.ku.dk/files/7371057/download?download_frd=1) or the embeddings that you want from this [website](https://fasttext.cc/docs/en/crawl-vectors.html). I recommend you downloading the file from Absalon, as it contains only 50,000 words and it is easier to load (well, faster).\n",
    "\n",
    "If you get the embeddings from the Github page, you should download the embeddings in **text** format. These embeddings have been trained with raw text from Wikipedia. This may take some space in your computer, depending on the language you choose.\n",
    "\n",
    "Once you downloaded the embeddings, it's time to start programming! The files follow a specific format.\n",
    "\n",
    "##### FILE FORMAT:\n",
    "\n",
    "The first line in the file contains two numbers separated by a single space. The first number indicates the number of words in the file (`N_WORDS`) and the second number specifies the number of dimensions (`N_DIMENSIONS`) that are used to represent each of those words.\n",
    "\n",
    "After the first line, each line will contain one word at the beginning. Following the word, and separated by spaces, there will be `N_DIMENSIONS` numbers, which represent each word in the space.\n",
    "\n",
    "The words are sorted by their frequency in the wikipedia corpus, then the first words in the file will be the most frequent ones. Here you can see how the English embeddings file starts:\n",
    "\n",
    "`9999 300`\n",
    "\n",
    "`, 0.1250 -0.1079 0.0245 -0.2529 0.1057 -0.0184 0.1177 ...`\n",
    "\n",
    "`the -0.0517 0.0740 -0.0131 0.0447 -0.0343 0.0212 0.0069 ...`\n",
    "\n",
    "`. 0.0342 -0.0801 0.1162 -0.3968 -0.0147 -0.0533 0.0606 ...`\n",
    "\n",
    "`and 0.0082 -0.0899 0.0265 -0.0086 -0.0609 0.0068 0.0652 ...`\n",
    "\n",
    "`...`\n",
    "\n",
    "##### What you have to do:\n",
    "\n",
    "Write a program to read the file and store the words and their embeddings in the data structure that you think it is the best. It might be very simple, or it might be a more complex one.\n",
    "\n",
    "##### Important note: You are not allowed to use a package like gensim to open the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "#1.- Define object to save words and their embeddings\n",
    "#2.- Write code for reading the file and save it in the defined object\n",
    "\n",
    "#YOUR CODE HERE\n",
    "import numpy as np\n",
    "word2vec = {} # create a dictionary to store words and their embeddings.\n",
    "with open('wiki.en.vec.short50K') as f:\n",
    "    word_number, vector_length = f.readline().split()\n",
    "    word_number = int(word_number)\n",
    "    vector_length = int(vector_length)\n",
    "    for line in f:\n",
    "        line_list = line.split()\n",
    "        if (len(line_list) != vector_length + 1): # process the embedding of ' '\n",
    "            word2vec[' '] = np.array([float(n) for n in line_list]) \n",
    "        else:\n",
    "            starting_index = line.find(line_list[1]) # handle duplications introduced by string.split()\n",
    "            word2vec[line[:starting_index - 1]] = np.array([float(n) for n in line_list[1:]]) \n",
    "    print(len(word2vec) == word_number)\n",
    "    print(len(word2vec['a']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2:\n",
    "\n",
    "A common distance metric used to measure the similarity between two words is the cosine similarity, which measures the cosine of the angle between the two vectors that represent each of the words.\n",
    "\n",
    "This similarity value is calculated by using this formula:\n",
    "\n",
    "$$\\text{similarity} = \\cos(\\theta) = {\\mathbf{A} \\cdot \\mathbf{B} \\over \\|\\mathbf{A}\\|_2 \\|\\mathbf{B}\\|_2} $$\n",
    "\n",
    "<!--= \\frac{ \\sum\\limits_{i=1}^{n}{A_i  B_i} }{ \\sqrt{\\sum\\limits_{i=1}^{n}{A_i^2}}  \\sqrt{\\sum\\limits_{i=1}^{n}{B_i^2}} }-->\n",
    "\n",
    "Don't be scared. The first part of the formula, $\\mathbf{A} \\cdot \\mathbf{B}$ is the dot product between vectors $\\mathbf{A}$ and $\\mathbf{B}$. And you know how to do that in Python.\n",
    "\n",
    "$\\mathbf{A} \\cdot \\mathbf{B} = \\sum\\limits_{i=1}^{n}{A_i  B_i}$\n",
    "\n",
    "In the lower part, $\\|\\mathbf{A}\\|_2 \\|\\mathbf{B}\\|_2$, you have to calculate the Euclidean norm of each vector ($\\mathbf{A}$ and $\\mathbf{B}$) and multiply their results. The Euclidean norm is calculated using this formula:\n",
    "\n",
    "$\\|\\mathbf{A}\\|_2 = \\sqrt{\\sum\\limits_{i=1}^{n}{A_i^2}}$\n",
    "\n",
    "The formula inside the square root is the same as the one from the dot product. Then it can be rewritten like this:\n",
    "\n",
    "$\\|\\mathbf{A}\\|_2 = \\sqrt{\\sum\\limits_{i=1}^{n}{A_i^2}} = \\sqrt{\\mathbf{A} \\cdot \\mathbf{A}} $\n",
    "\n",
    "You should program the cosine similarity function by using numpy. You cannot use previously programmed cosine similarity functions, you must write your own function. This program must get two numpy arrays and it should return a number.\n",
    "\n",
    "The resulting number of this formula should be interpreted as a number that specifies the similarity between two words. The higher the number, the similarity between those two words will be higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def similarity(A, B):\n",
    "    #YOUR CODE HERE\n",
    "    return np.dot(A, B) / (np.sqrt(np.dot(A, A)) * np.sqrt(np.dot(B, B)))\n",
    "similarity(np.array([1, 2, 3]), np.array([1, 2, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3:\n",
    "\n",
    "In the third exercise you have to squeeze your brain a bit more. Now, you have loaded the whole embedding file, and you also have a distance metric to measure the similarity between words. Let's do more complex things, then.\n",
    "\n",
    "Given a word, you have to find the 30 most similar words. Then, given one word you should get the distance to all the words in the embeddings file, and pick the nearest ones.\n",
    "\n",
    "In order to make this task easier, I attach a simple implementation of an ordered list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function should return the embeddings of a word according to your class\n",
    "def get (LIST, index):\n",
    "    return LIST[index]\n",
    "\n",
    "def get_value(el):\n",
    "    return el[1]\n",
    "\n",
    "\n",
    "\n",
    "class OrderedListTuple:\n",
    "    \n",
    "    def __init__(self, max_size):\n",
    "        self.content = []\n",
    "        self.max_size = max_size\n",
    "        \n",
    "    def find_pos (self, element):\n",
    "        index = 0\n",
    "        while (index <= len(self.content)-1) and get_value(get(self.content, index)) > get_value(element):\n",
    "            index += 1\n",
    "        return index\n",
    "\n",
    "    def insert_element (self, element):\n",
    "        pos = self.find_pos (element)\n",
    "        self.content.insert (pos, element)\n",
    "        if len(self.content) > self.max_size:\n",
    "            self.content.pop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation is very simple. When we initialize the list, we set the number of elements that it will have at most. Then, when we add elements to the list, it will add the element in the correct position. But, if the number of elements is higher than the ones that we can keep, the object will remove the last element. Let's see how it works with an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[('house', 14)]\n",
      "[('house', 14), ('home', 6)]\n",
      "[('house', 14), ('home', 6), ('brown', 3)]\n",
      "[('house', 14), ('home', 6), ('elbow', 4), ('brown', 3)]\n",
      "[('house', 14), ('home', 6), ('elbow', 4), ('brown', 3)]\n",
      "[('house', 14), ('the', 9), ('home', 6), ('elbow', 4)]\n",
      "[('and', 43), ('house', 14), ('the', 9), ('home', 6)]\n",
      "[('kitty', 44), ('and', 43), ('house', 14), ('the', 9)]\n"
     ]
    }
   ],
   "source": [
    "L = OrderedListTuple(4)\n",
    "print (L.content)\n",
    "\n",
    "L.insert_element((\"house\", 14))\n",
    "print (L.content)\n",
    "L.insert_element((\"home\", 6))\n",
    "print (L.content)\n",
    "L.insert_element((\"brown\", 3))\n",
    "print (L.content)\n",
    "L.insert_element((\"elbow\", 4))\n",
    "print (L.content)\n",
    "L.insert_element((\"high\", 1))\n",
    "print (L.content)\n",
    "L.insert_element((\"the\", 9))\n",
    "print (L.content)\n",
    "L.insert_element((\"and\", 43))\n",
    "print (L.content)\n",
    "L.insert_element((\"kitty\", 44))\n",
    "print (L.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hint: Why don't you create a similarity function that gets two words, and it returns a tuple? For each word in the dictionary, you can calculate the similarity to an input word, and save this in a tuple. Then, using the previous data structure, you can save only the N-best words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this data structure, you should be able to get the most similar words to one word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hi', 1.0), ('hello', 0.5977723609366353), ('teahouse', 0.5335621540492963), ('hey', 0.525167301428226), ('thankyou', 0.47517235995041396), ('thank', 0.472482012553265), ('welcome', 0.4669999708493511), ('yeh', 0.45915536871729445), ('haha', 0.4580143104819826), ('wondering', 0.4513551689838531), ('e/c', 0.4512011686221762), ('thanks', 0.45046507486020193), ('!', 0.44207923221814327), ('huh', 0.4366182393333329), ('guys', 0.43549323027198805), ('glad', 0.4301472480301115), ('fyi', 0.4276645430139309), ('sorry', 0.4265883328952608), ('btw', 0.424950023410508), ('heh', 0.42427025363994353), ('yeah', 0.41901551957353256), ('yep', 0.41813036657250763), ('lol', 0.4160081907654485), ('ho', 0.4145072555028499), ('ahh', 0.41330567490000225), ('nrg', 0.4113601767761329), ('thx', 0.411347409085337), ('jayron', 0.40702838256271623), ('okay', 0.40471536603795905), ('dear', 0.4044119726294408)]\n"
     ]
    }
   ],
   "source": [
    "#YOUR CODE HERE\n",
    "query_word = 'hi'\n",
    "nearest_30_words = OrderedListTuple(30)\n",
    "for word in word2vec.keys():\n",
    "    dis = similarity(word2vec[query_word], word2vec[word])\n",
    "    nearest_30_words.insert_element((word, dis))\n",
    "\n",
    "print(nearest_30_words.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4:\n",
    "\n",
    "The last exercise is really cool. One of the properties that researchers found in word embeddings was that we could perform algebraic operations over the vectors in order to get specific words.\n",
    "\n",
    "For example, if we perform an operation like this one:\n",
    "\n",
    "$$DICTIONARY['berlin'] - DICTIONARY['germany'] + DICTIONARY['france']$$\n",
    "\n",
    "This results in a vector. If we find the 20 closest words to that vector, we should be able to see that the word `Germany` will be near. Another nice operation was:\n",
    "\n",
    "$$DICTIONARY['queen'] - DICTIONARY['woman'] + DICTIONARY['man']$$\n",
    "\n",
    "Perform this operations with the words you want, and check if it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('paris', 0.7745821281649257)\n",
      "('berlin', 0.6806360501069306)\n",
      "('france', 0.6234364339620112)\n",
      "('marseille', 0.6068132959916013)\n",
      "('toulouse', 0.6053895238433366)\n",
      "('montpellier', 0.5917006165578015)\n",
      "('rouen', 0.5914574790560799)\n",
      "('avignon', 0.5890789796674709)\n",
      "('rennes', 0.5850185342314215)\n",
      "('ferrand', 0.584437310110859)\n",
      "('brussels', 0.5783981772734191)\n",
      "('nantes', 0.5750482980258821)\n",
      "('bordeaux', 0.5732881366656536)\n",
      "('marseilles', 0.5714683689465663)\n",
      "('boulogne', 0.5677972274128383)\n",
      "('neuilly', 0.5665287787178918)\n",
      "('grenoble', 0.5660398964645867)\n",
      "('lille', 0.5642594745563222)\n",
      "('provence', 0.5618433958267668)\n",
      "('poitiers', 0.5582482663698829)\n"
     ]
    }
   ],
   "source": [
    "#YOUR CODE HERE\n",
    "#berlin - germany + france\n",
    "new_vec = word2vec['berlin'] - word2vec['germany'] + word2vec['france']\n",
    "nearest_20_words = OrderedListTuple(20)\n",
    "for word in word2vec.keys():\n",
    "    dis = similarity(new_vec, word2vec[word])\n",
    "    nearest_20_words.insert_element((word, dis))\n",
    "\n",
    "for ele in nearest_20_words.content:\n",
    "    print(ele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('queen', 0.7898672082967787)\n",
      "('king', 0.6429407513059967)\n",
      "('majesty', 0.5372264198483584)\n",
      "('monarch', 0.5126070187949004)\n",
      "('crown', 0.469760687497465)\n",
      "('queens', 0.4569217657524951)\n",
      "('whitehall', 0.4508493631090601)\n",
      "('reign', 0.44983602827631264)\n",
      "('coronation', 0.44729696911956957)\n",
      "('royal', 0.43049023386023005)\n",
      "('regent', 0.42783651706315434)\n",
      "('jubilee', 0.42361938568326174)\n",
      "('kings', 0.4204812926368349)\n",
      "('prince', 0.41842767724878543)\n",
      "('connaught', 0.4147187331471937)\n",
      "('consort', 0.4101795096405899)\n",
      "('princess', 0.40904831868562314)\n",
      "('throne', 0.4079527608266969)\n",
      "('pretender', 0.40745142030398623)\n",
      "('elizabeth', 0.4070293809206282)\n"
     ]
    }
   ],
   "source": [
    "#YOUR CODE HERE\n",
    "#queen - woman + man\n",
    "new_vec = word2vec['queen'] - word2vec['woman'] + word2vec['man']\n",
    "nearest_20_words = OrderedListTuple(20)\n",
    "for word in word2vec.keys():\n",
    "    dis = similarity(new_vec, word2vec[word])\n",
    "    nearest_20_words.insert_element((word, dis))\n",
    "\n",
    "for ele in nearest_20_words.content:\n",
    "    print(ele)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 5:\n",
    "\n",
    "In recent years, many researchers have shown that word embeddings obtained from large corpora reproduce biases that happen in society.\n",
    "\n",
    "In this exercise, we would like to ask you to try to show some examples in the loaded word embedding file that show some sort of bias. This bias can be either of these, or any other bias you are interested:\n",
    "\n",
    " * Gender\n",
    " * Origin\n",
    " * Sexual preference\n",
    " * Socioeconomic class\n",
    " * Academic background\n",
    " \n",
    "These examples could be based on distances between words, but any other creative methodology that you could think of will be well considered as well.\n",
    "\n",
    "For instance, what is the distance between \"maid\" and \"man\", and \"maid\" and \"woman\"?\n",
    "\n",
    "You should provide examples but also your interpretation of these results.\n",
    "\n",
    "If you want to get some inspiration, you may want to check some recent articles about the topic:\n",
    "\n",
    "  * Bender, Emily M., and Batya Friedman. \"Data statements for natural language processing: Toward mitigating system bias and enabling better science.\" Transactions of the Association for Computational Linguistics 6 (2018): 587-604. https://aclanthology.org/Q18-1041/\n",
    "  * Hovy, Dirk, and Shrimai Prabhumoye. \"Five sources of bias in natural language processing.\" Language and Linguistics Compass 15.8 (2021): e12432. https://compass.onlinelibrary.wiley.com/doi/10.1111/lnc3.12432"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instance:\n",
      "0.4272625913611659\n",
      "0.6025203133248225\n",
      "Gender:\n",
      "0.40251331628816867\n",
      "0.24889993972693591\n",
      "Origen:\n",
      "0.31418397654228747\n",
      "0.09570860518933459\n",
      "Sexual preference:\n",
      "0.2779357787420189\n",
      "0.3390544125824317\n",
      "Socioeconomic class:\n",
      "0.24819745789478662\n",
      "0.10555141468182044\n",
      "Academic background:\n",
      "0.2926383594371341\n",
      "0.2669620695714518\n"
     ]
    }
   ],
   "source": [
    "#YOUR CODE HERE\n",
    "# instance:\n",
    "print('instance:')\n",
    "print(similarity(word2vec['maid'], word2vec['man']))\n",
    "print(similarity(word2vec['maid'], word2vec['woman']))\n",
    "# Gender\n",
    "print('Gender:') # From ChatGPT\n",
    "print(similarity(word2vec['nurse'], word2vec['she']))\n",
    "print(similarity(word2vec['nurse'], word2vec['he']))\n",
    "# Origen\n",
    "print('Origen:')\n",
    "print(similarity(word2vec['noodle'], word2vec['china']))\n",
    "print(similarity(word2vec['noodle'], word2vec['denmark']))\n",
    "# Sexual preference\n",
    "print('Sexual preference:')\n",
    "print(similarity(word2vec['queer'], word2vec['man']))\n",
    "print(similarity(word2vec['queer'], word2vec['woman']))\n",
    "# Socioeconomic class\n",
    "print('Socioeconomic class:')\n",
    "print(similarity(word2vec['porsche'], word2vec['elite']))\n",
    "print(similarity(word2vec['porsche'], word2vec['normal']))\n",
    "# Academic background\n",
    "print('Academic background:')\n",
    "print(similarity(word2vec['engineer'], word2vec['computer']))\n",
    "print(similarity(word2vec['engineer'], word2vec['building']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR INTERPRETATION HERE (press ENTER to write)  \n",
    "Basically, bias shows a stronger relation to certain words than other words. This is caused by the biased training data. And, bias usually represents stereotypes, prejudices, phenomenons, but also true connections.   \n",
    "- Gender\n",
    "'nurse' is closer to 'she'. This shows a fact that female burses are much more than male nurses, and shows a stereotype that when speaking of nurse, we might think it is a female nurse.  \n",
    "- Origen\n",
    "In this example, 'noodle' is closer to 'china' than 'denmark', which suggests that noodles might be more popular in China. \n",
    "- Sexual preference\n",
    "Ths distance between 'queer' and 'man' is lower than that of 'queer' and 'woman', which might suggests a prejudice.\n",
    "- Socioeconomic class\n",
    "'porsche' is a car brand and usually expensive. So, it is more related to 'elite' rather than 'normal' people.\n",
    "- Academic background\n",
    "In this case, the bias is not too obvious, but is still shows a perfection of 'computer'. This might suggest that there are more data that talks about engineers in computer science rather than Mechanical engineer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 6\n",
    "\n",
    "In this exercise we are going to see how to calculate word counts normalized by document frequency (TF-IDF).\n",
    "\n",
    "To this end, we will calculate the word frequencies and the IDF normalized counts using a Python package called TFIDFvectorizer from Scikit-Learn.\n",
    "\n",
    "We will work with the Gutenberg corpus from NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "import nltk\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileids = gutenberg.fileids()\n",
    "print(len(fileids))\n",
    "fileids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for fileid in fileids:\n",
    "    corpus.append(nltk.corpus.gutenberg.raw(fileid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18, 42063)\n",
      "12467 emma\n",
      "he 0.08135534295322783\n",
      "his 0.08280811693453548\n",
      "thee 0.08905556490592986\n",
      "with 0.09588308276630424\n",
      "my 0.1205802404485341\n",
      "to 0.16125791192514802\n",
      "in 0.20484113136437723\n",
      "of 0.21210500127091542\n",
      "and 0.5055653454950587\n",
      "the 0.6377677777940539\n"
     ]
    }
   ],
   "source": [
    "#Step 1: Learn vocabulary and idf, return document-term matrix.\n",
    "vec_tfidf = TfidfVectorizer() \n",
    "result_tfidf = vec_tfidf.fit_transform(corpus).toarray()\n",
    "print(result_tfidf.shape)\n",
    "#Step 2: Create id-to-word dictionary.\n",
    "id2word = {vec_tfidf.vocabulary_[key]:key for key in vec_tfidf.vocabulary_.keys()}\n",
    "for ite in id2word.items():\n",
    "    print(ite[0], ite[1])\n",
    "    break\n",
    "#Step 3: Sort the term frequencies of 'blake-poems.txt' and returns the sorted indexes\n",
    "sorted_ids_blake = np.argsort(result_tfidf[4]).reshape(-1)\n",
    "\n",
    "#Step 4: Print the top 10 TF-IDF scores and corresponding words. \n",
    "for id in sorted_ids_blake[-10:]:\n",
    "    print (id2word[id],result_tfidf[4][id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 6.1:\n",
    "- Explain using your own words and in one single sentence (per step) what happens in each step.\n",
    "###### Step 1: YOUR SENTENCE HERE\n",
    "- Learn vocabulary and idf, return document-term matrix.\n",
    "###### Step 2: YOUR SENTENCE HERE\n",
    "- Create id-to-word dictionary.\n",
    "###### Step 3: YOUR SENTENCE HERE\n",
    "- Sort the term frequencies of 'blake-poems.txt' and returns the sorted indexes.\n",
    "###### Step 4: YOUR SENTENCE HERE\n",
    "- Print the top 10 TF-IDF scores and corresponding words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 6.2:\n",
    "Can you check what are the top-10 most relevant words based on their inverse document frequency? I am asking for the 10 words with the highest inverse document frequency.\n",
    "\n",
    "If you do not know how to get the IDFs of the words, you may want to take a look at the documentation of the TFIDFvectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42063,)\n",
      "Top 10 most relevant words:\n",
      "3.2512917986064953 18404 hummingly\n",
      "3.2512917986064953 18402 hummer\n",
      "3.2512917986064953 18401 hummed\n",
      "3.2512917986064953 18399 humiliations\n",
      "3.2512917986064953 18396 humiliates\n",
      "3.2512917986064953 18392 humh\n",
      "3.2512917986064953 18390 humboldt\n",
      "3.2512917986064953 18388 humbling\n",
      "3.2512917986064953 18386 humblest\n",
      "3.2512917986064953 42062 zuzims\n"
     ]
    }
   ],
   "source": [
    "#YOUR CODE HERE\n",
    "idf_vector = vec_tfidf.idf_ # Get Idf\n",
    "print(idf_vector.shape)\n",
    "idf_vector_idx = np.argsort(idf_vector)\n",
    "print('Top 10 most relevant words:')\n",
    "for idx in idf_vector_idx[-10:]:\n",
    "    print(idf_vector[idx], idx, id2word[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2512917986064953 18331 howsoever\n",
      "3.2512917986064953 18385 humbles\n",
      "3.2512917986064953 18387 humbleth\n",
      "3.2512917986064953 18438 hungerbitten\n",
      "3.2512917986064953 18432 hundredfold\n",
      "3.2512917986064953 18430 hunchbacks\n",
      "3.2512917986064953 18429 hunchback\n",
      "3.2512917986064953 18428 hunch\n",
      "3.2512917986064953 18427 hun\n",
      "3.2512917986064953 18426 humtah\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# plt.figure(figsize=(12, 6.5))\n",
    "plt.hist(idf_vector, align = 'left', rwidth = 0.1)\n",
    "for idx in idf_vector_idx[-30:-20]:\n",
    "    print(idf_vector[idx], idx, id2word[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 6.3:\n",
    "\n",
    "How do the inverse document frequencies look like in this corpus? Do they seem relevant? Please state that in 1-2 sentences\n",
    "\n",
    "#### YOUR RESPONSE HERE\n",
    "- Lots of words have the idf value equal to 3.2512917986064953. \n",
    "- Some words with the same idf value are relevant, like hunchbacks, hunchback and hunch, but there are lots of irrelevant words even they have the same idf value. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
