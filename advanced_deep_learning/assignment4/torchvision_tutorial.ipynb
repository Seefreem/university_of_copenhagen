{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! wget https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip -P data\n",
    "# ! cd data && unzip PennFudanPed.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For tips on running notebooks in Google Colab, see\n",
    "# https://pytorch.org/tutorials/beginner/colab\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TorchVision Object Detection Finetuning Tutorial\n",
    "================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial, we will be finetuning a pre-trained [Mask\n",
    "R-CNN](https://arxiv.org/abs/1703.06870) model on the [Penn-Fudan\n",
    "Database for Pedestrian Detection and\n",
    "Segmentation](https://www.cis.upenn.edu/~jshi/ped_html/). It contains\n",
    "170 images with 345 instances of pedestrians, and we will use it to\n",
    "illustrate how to use the new features in torchvision in order to train\n",
    "an object detection and instance segmentation model on a custom dataset.\n",
    "\n",
    "Defining the Dataset\n",
    "====================\n",
    "\n",
    "The reference scripts for training object detection, instance\n",
    "segmentation and person keypoint detection allows for easily supporting\n",
    "adding new custom datasets. The dataset should inherit from the standard\n",
    "`torch.utils.data.Dataset`{.interpreted-text role=\"class\"} class, and\n",
    "implement `__len__` and `__getitem__`.\n",
    "\n",
    "The only specificity that we require is that the dataset `__getitem__`\n",
    "should return a tuple:\n",
    "\n",
    "-   image: `torchvision.tv_tensors.Image`{.interpreted-text\n",
    "    role=\"class\"} of shape `[3, H, W]`, a pure tensor, or a PIL Image of\n",
    "    size `(H, W)`\n",
    "-   target: a dict containing the following fields\n",
    "    -   `boxes`,\n",
    "        `torchvision.tv_tensors.BoundingBoxes`{.interpreted-text\n",
    "        role=\"class\"} of shape `[N, 4]`: the coordinates of the `N`\n",
    "        bounding boxes in `[x0, y0, x1, y1]` format, ranging from `0` to\n",
    "        `W` and `0` to `H`\n",
    "    -   `labels`, integer `torch.Tensor`{.interpreted-text role=\"class\"}\n",
    "        of shape `[N]`: the label for each bounding box. `0` represents\n",
    "        always the background class.\n",
    "    -   `image_id`, int: an image identifier. It should be unique\n",
    "        between all the images in the dataset, and is used during\n",
    "        evaluation\n",
    "    -   `area`, float `torch.Tensor`{.interpreted-text role=\"class\"} of\n",
    "        shape `[N]`: the area of the bounding box. This is used during\n",
    "        evaluation with the COCO metric, to separate the metric scores\n",
    "        between small, medium and large boxes.\n",
    "    -   `iscrowd`, uint8 `torch.Tensor`{.interpreted-text role=\"class\"}\n",
    "        of shape `[N]`: instances with `iscrowd=True` will be ignored\n",
    "        during evaluation.\n",
    "    -   (optionally) `masks`,\n",
    "        `torchvision.tv_tensors.Mask`{.interpreted-text role=\"class\"} of\n",
    "        shape `[N, H, W]`: the segmentation masks for each one of the\n",
    "        objects\n",
    "\n",
    "If your dataset is compliant with above requirements then it will work\n",
    "for both training and evaluation codes from the reference script.\n",
    "Evaluation code will use scripts from `pycocotools` which can be\n",
    "installed with `pip install pycocotools`.\n",
    "\n",
    "One note on the `labels`. The model considers class `0` as background.\n",
    "If your dataset does not contain the background class, you should not\n",
    "have `0` in your `labels`. For example, assuming you have just two\n",
    "classes, *cat* and *dog*, you can define `1` (not `0`) to represent\n",
    "*cats* and `2` to represent *dogs*. So, for instance, if one of the\n",
    "images has both classes, your `labels` tensor should look like `[1, 2]`.\n",
    "\n",
    "Additionally, if you want to use aspect ratio grouping during training\n",
    "(so that each batch only contains images with similar aspect ratios),\n",
    "then it is recommended to also implement a `get_height_and_width`\n",
    "method, which returns the height and the width of the image. If this\n",
    "method is not provided, we query all elements of the dataset via\n",
    "`__getitem__` , which loads the image in memory and is slower than if a\n",
    "custom method is provided.\n",
    "\n",
    "Writing a custom dataset for PennFudan\n",
    "--------------------------------------\n",
    "\n",
    "Let's write a dataset for the PennFudan dataset. First, let\\'s download\n",
    "the dataset and extract the [zip\n",
    "file](https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip):\n",
    "\n",
    "``` {.sourceCode .python}\n",
    "wget https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip -P data\n",
    "cd data && unzip PennFudanPed.zip\n",
    "```\n",
    "\n",
    "We have the following folder structure:\n",
    "\n",
    "    PennFudanPed/\n",
    "      PedMasks/\n",
    "        FudanPed00001_mask.png\n",
    "        FudanPed00002_mask.png\n",
    "        FudanPed00003_mask.png\n",
    "        FudanPed00004_mask.png\n",
    "        ...\n",
    "      PNGImages/\n",
    "        FudanPed00001.png\n",
    "        FudanPed00002.png\n",
    "        FudanPed00003.png\n",
    "        FudanPed00004.png\n",
    "\n",
    "Here is one example of a pair of images and segmentation masks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So each image has a corresponding segmentation mask, where each color\n",
    "correspond to a different instance. Let's write a\n",
    "`torch.utils.data.Dataset`{.interpreted-text role=\"class\"} class for\n",
    "this dataset. In the code below, we are wrapping images, bounding boxes\n",
    "and masks into `torchvision.tv_tensors.TVTensor`{.interpreted-text\n",
    "role=\"class\"} classes so that we will be able to apply torchvision\n",
    "built-in transformations ([new Transforms\n",
    "API](https://pytorch.org/vision/stable/transforms.html)) for the given\n",
    "object detection and segmentation task. Namely, image tensors will be\n",
    "wrapped by `torchvision.tv_tensors.Image`{.interpreted-text\n",
    "role=\"class\"}, bounding boxes into\n",
    "`torchvision.tv_tensors.BoundingBoxes`{.interpreted-text role=\"class\"}\n",
    "and masks into `torchvision.tv_tensors.Mask`{.interpreted-text\n",
    "role=\"class\"}. As `torchvision.tv_tensors.TVTensor`{.interpreted-text\n",
    "role=\"class\"} are `torch.Tensor`{.interpreted-text role=\"class\"}\n",
    "subclasses, wrapped objects are also tensors and inherit the plain\n",
    "`torch.Tensor`{.interpreted-text role=\"class\"} API. For more information\n",
    "about torchvision `tv_tensors` see [this\n",
    "documentation](https://pytorch.org/vision/main/auto_examples/transforms/plot_transforms_getting_started.html#what-are-tvtensors).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision.io import read_image\n",
    "from torchvision.ops.boxes import masks_to_boxes\n",
    "from torchvision import tv_tensors\n",
    "from torchvision.transforms.v2 import functional as F\n",
    "\n",
    "\n",
    "class PennFudanDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transforms):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        # load all image files, sorting them to\n",
    "        # ensure that they are aligned\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
    "        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load images and masks\n",
    "        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
    "        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
    "        img = read_image(img_path)\n",
    "        mask = read_image(mask_path)\n",
    "        # instances are encoded as different colors\n",
    "        obj_ids = torch.unique(mask)\n",
    "        # first id is the background, so remove it\n",
    "        obj_ids = obj_ids[1:]\n",
    "        num_objs = len(obj_ids)\n",
    "\n",
    "        # split the color-encoded mask into a set\n",
    "        # of binary masks\n",
    "        masks = (mask == obj_ids[:, None, None]).to(dtype=torch.uint8)\n",
    "\n",
    "        # get bounding box coordinates for each mask\n",
    "        boxes = masks_to_boxes(masks)\n",
    "\n",
    "        # there is only one class\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        image_id = idx\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        # Wrap sample and targets into torchvision tv_tensors:\n",
    "        img = tv_tensors.Image(img)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = tv_tensors.BoundingBoxes(boxes, format=\"XYXY\", canvas_size=F.get_size(img))\n",
    "        target[\"masks\"] = tv_tensors.Mask(masks)\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all for the dataset. Now let's define a model that can perform\n",
    "predictions on this dataset.\n",
    "\n",
    "Defining your model\n",
    "===================\n",
    "\n",
    "In this tutorial, we will be using [Mask\n",
    "R-CNN](https://arxiv.org/abs/1703.06870), which is based on top of\n",
    "[Faster R-CNN](https://arxiv.org/abs/1506.01497). Faster R-CNN is a\n",
    "model that predicts both bounding boxes and class scores for potential\n",
    "objects in the image.\n",
    "\n",
    "![image](https://pytorch.org/tutorials/_static/img/tv_tutorial/tv_image03.png)\n",
    "\n",
    "Mask R-CNN adds an extra branch into Faster R-CNN, which also predicts\n",
    "segmentation masks for each instance.\n",
    "\n",
    "![image](https://pytorch.org/tutorials/_static/img/tv_tutorial/tv_image04.png)\n",
    "\n",
    "There are two common situations where one might want to modify one of\n",
    "the available models in TorchVision Model Zoo. The first is when we want\n",
    "to start from a pre-trained model, and just finetune the last layer. The\n",
    "other is when we want to replace the backbone of the model with a\n",
    "different one (for faster predictions, for example).\n",
    "\n",
    "Let's go see how we would do one or another in the following sections.\n",
    "\n",
    "1 - Finetuning from a pretrained model\n",
    "--------------------------------------\n",
    "\n",
    "Let's suppose that you want to start from a model pre-trained on COCO\n",
    "and want to finetune it for your particular classes. Here is a possible\n",
    "way of doing it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "# load a model pre-trained on COCO\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "\n",
    "# replace the classifier with a new one, that has\n",
    "# num_classes which is user-defined\n",
    "num_classes = 2  # 1 class (person) + background\n",
    "# get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Object detection and instance segmentation model for PennFudan Dataset\n",
    "======================================================================\n",
    "\n",
    "In our case, we want to finetune from a pre-trained model, given that\n",
    "our dataset is very small, so we will be following approach number 1.\n",
    "\n",
    "Here we want to also compute the instance segmentation masks, so we will\n",
    "be using Mask R-CNN:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "\n",
    "def get_model_instance_segmentation(num_classes):\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # # now get the number of input features for the mask classifier\n",
    "    # in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    # hidden_layer = 256\n",
    "    # # and replace the mask predictor with a new one\n",
    "    # model.roi_heads.mask_predictor = MaskRCNNPredictor(\n",
    "    #     in_features_mask,\n",
    "    #     hidden_layer,\n",
    "    #     num_classes\n",
    "    # )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it, this will make `model` be ready to be trained and evaluated\n",
    "on your custom dataset.\n",
    "\n",
    "Putting everything together\n",
    "===========================\n",
    "\n",
    "In `references/detection/`, we have a number of helper functions to\n",
    "simplify training and evaluating detection models. Here, we will use\n",
    "`references/detection/engine.py` and `references/detection/utils.py`.\n",
    "Just download everything under `references/detection` to your folder and\n",
    "use them here. On Linux if you have `wget`, you can download them using\n",
    "below commands:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2024-06-12 12:13:16--  https://raw.githubusercontent.com/pytorch/vision/main/references/detection/engine.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.84.133, 151.101.76.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.84.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4063 (4.0K) [text/plain]\n",
      "Saving to: ‘engine.py.1’\n",
      "\n",
      "     0K ...                                                   100% 11.8M=0s\n",
      "\n",
      "2024-06-12 12:13:16 (11.8 MB/s) - ‘engine.py.1’ saved [4063/4063]\n",
      "\n",
      "--2024-06-12 12:13:16--  https://raw.githubusercontent.com/pytorch/vision/main/references/detection/utils.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.84.133, 151.101.76.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.84.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 8388 (8.2K) [text/plain]\n",
      "Saving to: ‘utils.py.1’\n",
      "\n",
      "     0K ........                                              100% 5.77M=0.001s\n",
      "\n",
      "2024-06-12 12:13:17 (5.77 MB/s) - ‘utils.py.1’ saved [8388/8388]\n",
      "\n",
      "--2024-06-12 12:13:17--  https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_utils.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.84.133, 151.101.76.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.84.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 8397 (8.2K) [text/plain]\n",
      "Saving to: ‘coco_utils.py.1’\n",
      "\n",
      "     0K ........                                              100% 5.30M=0.002s\n",
      "\n",
      "2024-06-12 12:13:17 (5.30 MB/s) - ‘coco_utils.py.1’ saved [8397/8397]\n",
      "\n",
      "--2024-06-12 12:13:17--  https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_eval.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.84.133, 151.101.76.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.84.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6447 (6.3K) [text/plain]\n",
      "Saving to: ‘coco_eval.py.1’\n",
      "\n",
      "     0K ......                                                100% 4.74M=0.001s\n",
      "\n",
      "2024-06-12 12:13:17 (4.74 MB/s) - ‘coco_eval.py.1’ saved [6447/6447]\n",
      "\n",
      "--2024-06-12 12:13:17--  https://raw.githubusercontent.com/pytorch/vision/main/references/detection/transforms.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.84.133, 151.101.76.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.84.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 23628 (23K) [text/plain]\n",
      "Saving to: ‘transforms.py.1’\n",
      "\n",
      "     0K .......... .......... ...                             100% 5.58M=0.004s\n",
      "\n",
      "2024-06-12 12:13:18 (5.58 MB/s) - ‘transforms.py.1’ saved [23628/23628]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/engine.py\")\n",
    "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/utils.py\")\n",
    "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_utils.py\")\n",
    "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_eval.py\")\n",
    "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/transforms.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since v0.15.0 torchvision provides [new Transforms\n",
    "API](https://pytorch.org/vision/stable/transforms.html) to easily write\n",
    "data augmentation pipelines for Object Detection and Segmentation tasks.\n",
    "\n",
    "Let's write some helper functions for data augmentation /\n",
    "transformation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2 as T\n",
    "\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    if train:\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    transforms.append(T.ToDtype(torch.float, scale=True))\n",
    "    transforms.append(T.ToPureTensor())\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing `forward()` method (Optional)\n",
    "=====================================\n",
    "\n",
    "Before iterating over the dataset, it\\'s good to see what the model\n",
    "expects during training and inference time on sample data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "dataset = PennFudanDataset('data/PennFudanPed', get_transform(train=True))\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    collate_fn=utils.collate_fn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'boxes': tensor([[ 85.,  72., 170., 359.]]),\n",
       "  'masks': tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           ...,\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8),\n",
       "  'labels': tensor([1]),\n",
       "  'image_id': 112,\n",
       "  'area': tensor([24395.]),\n",
       "  'iscrowd': tensor([0])},\n",
       " {'boxes': tensor([[ 40., 107., 178., 384.],\n",
       "          [299., 107., 384., 383.]]),\n",
       "  'masks': tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           ...,\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0]],\n",
       "  \n",
       "          [[0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           ...,\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8),\n",
       "  'labels': tensor([1, 1]),\n",
       "  'image_id': 5,\n",
       "  'area': tensor([38226., 23460.]),\n",
       "  'iscrowd': tensor([0, 0])}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images, targets = next(iter(data_loader))\n",
    "images = list(image for image in images)\n",
    "targets = [{k: v for k, v in t.items()} for t in targets]\n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss_classifier': tensor(0.0288, grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(0.0472, grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.0101, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(0.0014, grad_fn=<DivBackward0>)}\n",
      "{'boxes': tensor([], size=(0, 4), grad_fn=<StackBackward0>), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([], grad_fn=<IndexBackward0>)}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "dataset = PennFudanDataset('data/PennFudanPed', get_transform(train=True))\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    collate_fn=utils.collate_fn\n",
    ")\n",
    "\n",
    "# For Training\n",
    "images, targets = next(iter(data_loader))\n",
    "images = list(image for image in images)\n",
    "targets = [{k: v for k, v in t.items()} for t in targets]\n",
    "output = model(images, targets)  # Returns losses and detections\n",
    "print(output)\n",
    "\n",
    "# For inference\n",
    "model.eval()\n",
    "x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
    "predictions = model(x)  # Returns predictions\n",
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now write the main function which performs the training and the\n",
    "validation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pycocotools in /home/seelur/enter/lib/python3.11/site-packages (2.0.7)\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in /home/seelur/enter/lib/python3.11/site-packages (from pycocotools) (3.7.1)\n",
      "Requirement already satisfied: numpy in /home/seelur/enter/lib/python3.11/site-packages (from pycocotools) (1.24.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/seelur/enter/lib/python3.11/site-packages (from matplotlib>=2.1.0->pycocotools) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/seelur/enter/lib/python3.11/site-packages (from matplotlib>=2.1.0->pycocotools) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/seelur/enter/lib/python3.11/site-packages (from matplotlib>=2.1.0->pycocotools) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/seelur/enter/lib/python3.11/site-packages (from matplotlib>=2.1.0->pycocotools) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/seelur/enter/lib/python3.11/site-packages (from matplotlib>=2.1.0->pycocotools) (23.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/seelur/enter/lib/python3.11/site-packages (from matplotlib>=2.1.0->pycocotools) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/seelur/enter/lib/python3.11/site-packages (from matplotlib>=2.1.0->pycocotools) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/seelur/enter/lib/python3.11/site-packages (from matplotlib>=2.1.0->pycocotools) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/seelur/enter/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install pycocotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from engine import train_one_epoch, evaluate\n",
    "\n",
    "# train on the GPU or on the CPU, if a GPU is not available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# our dataset has two classes only - background and person\n",
    "num_classes = 2\n",
    "# use our dataset and defined transformations\n",
    "dataset = PennFudanDataset('data/PennFudanPed', get_transform(train=True))\n",
    "dataset_test = PennFudanDataset('data/PennFudanPed', get_transform(train=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.PennFudanDataset"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# split the dataset in train and test set\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
    "dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    collate_fn=utils.collate_fn\n",
    ")\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    collate_fn=utils.collate_fn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.utils.data.dataset.Subset"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [ 0/60]  eta: 0:01:39  lr: 0.000090  loss: 0.9609 (0.9609)  loss_classifier: 0.5935 (0.5935)  loss_box_reg: 0.1613 (0.1613)  loss_mask: 0.1333 (0.1333)  loss_objectness: 0.0719 (0.0719)  loss_rpn_box_reg: 0.0009 (0.0009)  time: 1.6513  data: 0.0303  max mem: 1773\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 28\u001b[0m\n\u001b[1;32m     24\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# train for one epoch, printing every 10 iterations\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m     train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# update the learning rate\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/git/UCPH/git_backups/university_of_copenhagen/advanced_deep_learning/assignment4/engine.py:27\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, optimizer, data_loader, device, epoch, print_freq, scaler)\u001b[0m\n\u001b[1;32m     21\u001b[0m     warmup_iters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;241m1000\u001b[39m, \u001b[38;5;28mlen\u001b[39m(data_loader) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     23\u001b[0m     lr_scheduler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mLinearLR(\n\u001b[1;32m     24\u001b[0m         optimizer, start_factor\u001b[38;5;241m=\u001b[39mwarmup_factor, total_iters\u001b[38;5;241m=\u001b[39mwarmup_iters\n\u001b[1;32m     25\u001b[0m     )\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, targets \u001b[38;5;129;01min\u001b[39;00m metric_logger\u001b[38;5;241m.\u001b[39mlog_every(data_loader, print_freq, header):\n\u001b[1;32m     28\u001b[0m     images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(image\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images)\n\u001b[1;32m     29\u001b[0m     targets \u001b[38;5;241m=\u001b[39m [{k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m targets]\n",
      "File \u001b[0;32m~/git/UCPH/git_backups/university_of_copenhagen/advanced_deep_learning/assignment4/utils.py:171\u001b[0m, in \u001b[0;36mMetricLogger.log_every\u001b[0;34m(self, iterable, print_freq, header)\u001b[0m\n\u001b[1;32m    167\u001b[0m     log_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelimiter\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m    168\u001b[0m         [header, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m space_fmt \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m}/\u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meta: \u001b[39m\u001b[38;5;132;01m{eta}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{meters}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime: \u001b[39m\u001b[38;5;132;01m{time}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata: \u001b[39m\u001b[38;5;132;01m{data}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    169\u001b[0m     )\n\u001b[1;32m    170\u001b[0m MB \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1024.0\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1024.0\u001b[39m\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m    172\u001b[0m     data_time\u001b[38;5;241m.\u001b[39mupdate(time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m end)\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m obj\n",
      "File \u001b[0;32m~/enter/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/enter/lib/python3.11/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/enter/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/enter/lib/python3.11/site-packages/torch/utils/data/dataset.py:399\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/enter/lib/python3.11/site-packages/torch/utils/data/dataset.py:399\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[0;32mIn[3], line 52\u001b[0m, in \u001b[0;36mPennFudanDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     50\u001b[0m target \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     51\u001b[0m target[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboxes\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m tv_tensors\u001b[38;5;241m.\u001b[39mBoundingBoxes(boxes, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXYXY\u001b[39m\u001b[38;5;124m\"\u001b[39m, canvas_size\u001b[38;5;241m=\u001b[39mF\u001b[38;5;241m.\u001b[39mget_size(img))\n\u001b[0;32m---> 52\u001b[0m target[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmasks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m tv_tensors\u001b[38;5;241m.\u001b[39mMask(masks)\n\u001b[1;32m     53\u001b[0m target[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m labels\n\u001b[1;32m     54\u001b[0m target[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_id\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m image_id\n",
      "File \u001b[0;32m~/enter/lib/python3.11/site-packages/torchvision/tv_tensors/_mask.py:38\u001b[0m, in \u001b[0;36mMask.__new__\u001b[0;34m(cls, data, dtype, device, requires_grad)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m functional \u001b[38;5;28;01mas\u001b[39;00m F\n\u001b[1;32m     36\u001b[0m     data \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpil_to_tensor(data)\n\u001b[0;32m---> 38\u001b[0m tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_to_tensor(data, dtype\u001b[38;5;241m=\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mdevice, requires_grad\u001b[38;5;241m=\u001b[39mrequires_grad)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mas_subclass(\u001b[38;5;28mcls\u001b[39m)\n",
      "File \u001b[0;32m~/enter/lib/python3.11/site-packages/torchvision/tv_tensors/_tv_tensor.py:23\u001b[0m, in \u001b[0;36mTVTensor._to_tensor\u001b[0;34m(data, dtype, device, requires_grad)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mTVTensor\u001b[39;00m(torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m     16\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Base class for all TVTensors.\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m    You probably don't want to use this class unless you're defining your own\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m    custom TVTensors. See\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m    :ref:`sphx_glr_auto_examples_transforms_plot_custom_tv_tensors.py` for details.\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_tensor\u001b[39m(\n\u001b[1;32m     25\u001b[0m         data: Any,\n\u001b[1;32m     26\u001b[0m         dtype: Optional[torch\u001b[38;5;241m.\u001b[39mdtype] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     27\u001b[0m         device: Optional[Union[torch\u001b[38;5;241m.\u001b[39mdevice, \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     28\u001b[0m         requires_grad: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     29\u001b[0m     ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m requires_grad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     31\u001b[0m             requires_grad \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# get the model using our helper function\n",
    "model = get_model_instance_segmentation(num_classes)\n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(\n",
    "    params,\n",
    "    lr=0.005,\n",
    "    momentum=0.9,\n",
    "    weight_decay=0.0005\n",
    ")\n",
    "\n",
    "# and a learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=3,\n",
    "    gamma=0.1\n",
    ")\n",
    "\n",
    "# let's train it just for 2 epochs\n",
    "num_epochs = 2\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "    evaluate(model, data_loader_test, device=device)\n",
    "\n",
    "print(\"That's it!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So after one epoch of training, we obtain a COCO-style mAP \\> 50, and a\n",
    "mask mAP of 65.\n",
    "\n",
    "But what do the predictions look like? Let's take one image in the\n",
    "dataset and verify\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision.utils import draw_bounding_boxes, draw_segmentation_masks\n",
    "\n",
    "\n",
    "image = read_image(\"data/PennFudanPed/PNGImages/FudanPed00046.png\")\n",
    "eval_transform = get_transform(train=False)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    x = eval_transform(image)\n",
    "    # convert RGBA -> RGB and move to device\n",
    "    x = x[:3, ...].to(device)\n",
    "    predictions = model([x, ])\n",
    "    pred = predictions[0]\n",
    "\n",
    "\n",
    "image = (255.0 * (image - image.min()) / (image.max() - image.min())).to(torch.uint8)\n",
    "image = image[:3, ...]\n",
    "pred_labels = [f\"pedestrian: {score:.3f}\" for label, score in zip(pred[\"labels\"], pred[\"scores\"])]\n",
    "pred_boxes = pred[\"boxes\"].long()\n",
    "output_image = draw_bounding_boxes(image, pred_boxes, pred_labels, colors=\"red\")\n",
    "\n",
    "masks = (pred[\"masks\"] > 0.7).squeeze(1)\n",
    "output_image = draw_segmentation_masks(output_image, masks, alpha=0.5, colors=\"blue\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(output_image.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results look good!\n",
    "\n",
    "Wrapping up\n",
    "===========\n",
    "\n",
    "In this tutorial, you have learned how to create your own training\n",
    "pipeline for object detection models on a custom dataset. For that, you\n",
    "wrote a `torch.utils.data.Dataset`{.interpreted-text role=\"class\"} class\n",
    "that returns the images and the ground truth boxes and segmentation\n",
    "masks. You also leveraged a Mask R-CNN model pre-trained on COCO\n",
    "train2017 in order to perform transfer learning on this new dataset.\n",
    "\n",
    "For a more complete example, which includes multi-machine / multi-GPU\n",
    "training, check `references/detection/train.py`, which is present in the\n",
    "torchvision repository.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
