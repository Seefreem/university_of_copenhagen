{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2c0abd51",
      "metadata": {
        "id": "2c0abd51"
      },
      "source": [
        "# Feedforward classification of the NMIST data\n",
        "### Advanced Deep Learning 2024\n",
        "This notebook was written originally Jon Sporring (mailto:sporring@di.ku.dk) and heavily inspired by https://clay-atlas.com/us/blog/2021/04/22/pytorch-en-tutorial-4-train-a-model-to-classify-mnist.\n",
        "\n",
        "We consider the Modified National Institute of Standards and Technology database of handwritten digits (MNIST): http://yann.lecun.com/exdb/mnist/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4b1cd80",
      "metadata": {
        "id": "a4b1cd80"
      },
      "source": [
        "## Installs\n",
        "\n",
        "On non-colab system, is usually good to make an environment and install necessary tools there. E.g., anaconda->jupyter->terminal create an environment, if you have not already, and activate it:\n",
        "```\n",
        "conda create -n adl python=3.9\n",
        "conda activate adl\n",
        "```\n",
        "then install missing packages such as:\n",
        "```\n",
        "conda install ipykernel torch matplotlib torchmetrics scikit-image jpeg\n",
        "conda install -c conda-forge segmentation-models-pytorch ipywidgets\n",
        "```\n",
        "and if you want to add it to jupyter's drop-down menu\n",
        "```\n",
        "ipython kernel install --user --name=adl\n",
        "```\n",
        "Now reload the jupyter-notebook's homepage and make a new or load an existing file. On colab, the tools have to be installed everytime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "4afe90fa",
      "metadata": {
        "id": "4afe90fa"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "if IN_COLAB:\n",
        "    !pip3 install torch matplotlib torchmetrics scikit-image segmentation-models-pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0112f199",
      "metadata": {
        "id": "0112f199"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "372f3da4",
      "metadata": {
        "id": "372f3da4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as dset\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "709fa153",
      "metadata": {
        "id": "709fa153"
      },
      "source": [
        "## Set global device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "ef384f3a",
      "metadata": {
        "id": "ef384f3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU State: cuda:0\n"
          ]
        }
      ],
      "source": [
        "# GPU\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "print('GPU State:', device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0479fa7",
      "metadata": {
        "id": "c0479fa7"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "16d728ef",
      "metadata": {
        "id": "16d728ef"
      },
      "outputs": [],
      "source": [
        "def training_loop(model, loss, optimizer, loader, epochs, verbose=True, device=device):\n",
        "    \"\"\"\n",
        "    Run training of a model given a loss function, optimizer and a set of training and validation data.\n",
        "    \"\"\"\n",
        "\n",
        "    # Train\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for times, data in enumerate(loader):\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "            # inputs = inputs.view(inputs.shape[0], -1)\n",
        "\n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Foward + backward + optimize\n",
        "            outputs = model(inputs)\n",
        "            loss_tensor = loss(outputs, labels)\n",
        "            loss_tensor.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Print statistics\n",
        "            running_loss += loss_tensor.item()\n",
        "            if verbose:\n",
        "                if times % 100 == 99 or times+1 == len(loader):\n",
        "                    print('[%d/%d, %d/%d] loss: %.3f' % (epoch+1, epochs, times+1, len(loader), running_loss/2000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "2d7ce08f",
      "metadata": {
        "id": "2d7ce08f"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, loader, device=device):\n",
        "    \"\"\"\n",
        "    Evaluate a model 'model' on all batches of a torch DataLoader 'data_loader'.\n",
        "\n",
        "    Returns: the total number of correct classifications,\n",
        "             the total number of images\n",
        "             the list of the per class correct classification,\n",
        "             the list of the per class total number of images.\n",
        "    \"\"\"\n",
        "\n",
        "    # Test\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            # inputs = inputs.view(inputs.shape[0], -1)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    class_correct = [0 for i in range(10)]\n",
        "    class_total = [0 for i in range(10)]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "            # inputs = inputs.view(inputs.shape[0], -1)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            c = (predicted == labels).squeeze()\n",
        "            for i in range(10):\n",
        "                label = labels[i]\n",
        "                class_correct[label] += c[i].item()\n",
        "                class_total[label] += 1\n",
        "\n",
        "    return (correct, total, class_correct, class_total)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd353eee",
      "metadata": {
        "id": "cd353eee"
      },
      "source": [
        "## Main program"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "142ae598",
      "metadata": {
        "id": "142ae598"
      },
      "outputs": [],
      "source": [
        "# Transform\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5,), (0.5,)),]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "94591bc3",
      "metadata": {
        "id": "94591bc3"
      },
      "outputs": [],
      "source": [
        "# Data\n",
        "trainSet = datasets.MNIST(root='MNIST', download=True, train=True, transform=transform)\n",
        "testSet = datasets.MNIST(root='MNIST', download=True, train=False, transform=transform)\n",
        "trainLoader = dset.DataLoader(trainSet, batch_size=64, shuffle=True)\n",
        "testLoader = dset.DataLoader(testSet, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "cf8845d3",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 28, 28])"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainSet[0][0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "848e7dcc",
      "metadata": {
        "id": "848e7dcc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (main): Sequential(\n",
            "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (4): ReLU()\n",
            "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (6): Flatten(start_dim=1, end_dim=-1)\n",
            "    (7): Linear(in_features=800, out_features=10, bias=True)\n",
            "    (8): LogSoftmax(dim=1)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size =(3, 3), stride =(1, 1)),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size =2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
        "            nn.Conv2d (16, 32, kernel_size =(3, 3), stride =(1, 1)),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size =2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
        "            nn.Flatten(start_dim=1, end_dim =-1),\n",
        "            nn.Linear(800, out_features =10, bias=True),\n",
        "            nn.LogSoftmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)\n",
        "\n",
        "\n",
        "net = Net().to(device)\n",
        "print(net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "fce0735b",
      "metadata": {
        "id": "fce0735b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on 60000 images\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/4, 100/938] loss: 0.095\n",
            "[1/4, 200/938] loss: 0.125\n",
            "[1/4, 300/938] loss: 0.143\n",
            "[1/4, 400/938] loss: 0.157\n",
            "[1/4, 500/938] loss: 0.170\n",
            "[1/4, 600/938] loss: 0.181\n",
            "[1/4, 700/938] loss: 0.192\n",
            "[1/4, 800/938] loss: 0.201\n",
            "[1/4, 900/938] loss: 0.210\n",
            "[1/4, 938/938] loss: 0.213\n",
            "[2/4, 100/938] loss: 0.008\n",
            "[2/4, 200/938] loss: 0.015\n",
            "[2/4, 300/938] loss: 0.022\n",
            "[2/4, 400/938] loss: 0.029\n",
            "[2/4, 500/938] loss: 0.035\n",
            "[2/4, 600/938] loss: 0.042\n",
            "[2/4, 700/938] loss: 0.048\n",
            "[2/4, 800/938] loss: 0.054\n",
            "[2/4, 900/938] loss: 0.060\n",
            "[2/4, 938/938] loss: 0.062\n",
            "[3/4, 100/938] loss: 0.005\n",
            "[3/4, 200/938] loss: 0.010\n",
            "[3/4, 300/938] loss: 0.015\n",
            "[3/4, 400/938] loss: 0.021\n",
            "[3/4, 500/938] loss: 0.026\n",
            "[3/4, 600/938] loss: 0.030\n",
            "[3/4, 700/938] loss: 0.035\n",
            "[3/4, 800/938] loss: 0.039\n",
            "[3/4, 900/938] loss: 0.044\n",
            "[3/4, 938/938] loss: 0.045\n",
            "[4/4, 100/938] loss: 0.004\n",
            "[4/4, 200/938] loss: 0.008\n",
            "[4/4, 300/938] loss: 0.013\n",
            "[4/4, 400/938] loss: 0.017\n",
            "[4/4, 500/938] loss: 0.020\n",
            "[4/4, 600/938] loss: 0.024\n",
            "[4/4, 700/938] loss: 0.028\n",
            "[4/4, 800/938] loss: 0.032\n",
            "[4/4, 900/938] loss: 0.036\n",
            "[4/4, 938/938] loss: 0.037\n",
            "Training Finished.\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 97 %\n",
            "Accuracy of 0: 0.993421\n",
            "Accuracy of 1: 0.983784\n",
            "Accuracy of 2: 0.988372\n",
            "Accuracy of 3: 0.961538\n",
            "Accuracy of 4: 0.988701\n",
            "Accuracy of 5: 0.976190\n",
            "Accuracy of 6: 0.968750\n",
            "Accuracy of 7: 0.945122\n",
            "Accuracy of 8: 0.965035\n",
            "Accuracy of 9: 0.976048\n"
          ]
        }
      ],
      "source": [
        "# Parameters\n",
        "epochs = 4\n",
        "lr = 0.002\n",
        "loss = nn.NLLLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.002, momentum=0.9)\n",
        "\n",
        "# Train\n",
        "print('Training on %d images' % trainSet.data.shape[0])\n",
        "training_loop(net, loss, optimizer, trainLoader, epochs)\n",
        "print('Training Finished.\\n')\n",
        "\n",
        "# Test\n",
        "correct, total, class_correct, class_total = evaluate_model(net, testLoader)\n",
        "print('Accuracy of the network on the %d test images: %d %%' % (testSet.data.shape[0], (100*correct / total)))\n",
        "for i in range(10):\n",
        "\n",
        "    print('Accuracy of %d: %3f' % (i, (class_correct[i]/class_total[i])))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b541c029",
      "metadata": {},
      "source": [
        "Accuracy of the network on the 10000 test images: 97 %   \n",
        "Accuracy of 0: 0.993421   \n",
        "Accuracy of 1: 0.983784   \n",
        "Accuracy of 2: 0.988372   \n",
        "Accuracy of 3: 0.961538   \n",
        "Accuracy of 4: 0.988701   \n",
        "Accuracy of 5: 0.976190   \n",
        "Accuracy of 6: 0.968750   \n",
        "Accuracy of 7: 0.945122   \n",
        "Accuracy of 8: 0.965035   \n",
        "Accuracy of 9: 0.976048   \n",
        "\n",
        "- Why must the Linear layer be preceded by Flatten?   \n",
        "    A: The outputs of the last convolutional layer are usually not an one dimensional tensor,   \n",
        "    but a three dimensional tensor. However, the Linear layer only takes an one dimensional tensor as an input. \n",
        "- Why does it have 800 input features?  \n",
        "    A: All the calculations are accorded to the equations in https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html and https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html.  \n",
        "    Input image shape: 1x28x28  \n",
        "    After the first convolutional layer: 16x26x26  \n",
        "    After the first pooling layer: 16x13x13  \n",
        "    After the second convolutional layer: 32x11x11  \n",
        "    After the second pooling layer: 32x5x5  \n",
        "    Flattened: 32x5x5 = 800  \n",
        "    The flattened tensor has 800 elements, therefore, the input of the linear layer has 800 features.  \n",
        "- How well does it classify digits as compared to the feed-forward network?   \n",
        "    Better  \n",
        "- What are its number parameters relative to the feed-forward network?  \n",
        "    The formula to calculate the number of parameters in a convolutional layer is:  \n",
        "    Number of Parameters=(Kernel Height×Kernel Width×Input Channels+1)×Output Channels  \n",
        "    The number of parameters:  \n",
        "    (3*3*1+1)*16+(3*3*16+1)*32+800*10+10 = 12810, fewer than that of the feed-forward network.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "7f06850a",
      "metadata": {
        "id": "7f06850a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameters: 12810\n"
          ]
        }
      ],
      "source": [
        "print('Parameters:', sum(p.numel() for p in net.parameters()))\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
